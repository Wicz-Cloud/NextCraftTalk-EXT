# Port Configuration
BOT_PORT=8111                    # Port for bot API (FastAPI)
OLLAMA_PORT=11434               # Port for Ollama service

# Nextcloud Configuration
NEXTCLOUD_URL=https://your-nextcloud.example.com  # Your Nextcloud instance URL
NEXTCLOUD_BOT_TOKEN=your-bot-token-here          # Bot token from Nextcloud admin settings
SHARED_SECRET=your-shared-secret-here           # Webhook signature verification (from Nextcloud AIO)

# Bot Configuration
BOT_NAME=MinecraftBot            # Bot username in Nextcloud
BOT_DISPLAY_NAME=Minecraft Helper # Bot display name

# LLM Configuration
OLLAMA_URL=http://ollama:11434   # Ollama service URL (internal Docker network)
MODEL_NAME=gemma2:2b            # LLM model to use (phi3:mini, gemma2:2b, mistral:7b-instruct)

# Prompt Template Configuration
PROMPT_TEMPLATE_PATH=prompt_template.txt  # Path to external prompt template file

# Performance Configuration
TOP_K_RESULTS=2                 # Number of documents to retrieve for RAG
BATCH_SIZE=50                   # Batch size for processing

# Docker Network Configuration
NETWORK_NAME=nextcloud-aio      # Docker network name (nextcloud-aio for AIO, bridge for standalone)

# Alternative model options:
# MODEL_NAME=gemma2:2b
# MODEL_NAME=mistral:7b-instruct
# MODEL_NAME=llama3:8b-instruct

# Performance Settings (for Raspberry Pi)
MAX_WORKERS=2
BATCH_SIZE=50
